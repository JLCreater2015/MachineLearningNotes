---
title: 熵与交叉熵
tags: 熵,交叉熵,逻辑回归,Softmax
grammar_cjkRuby: true
---

[TOC]


#### 1、熵（信息熵）
熵（entropy）是信息论中最基本、最核心的一个概念，它衡量了一个概率分布的随机程度，或者说包含的信息量的大小。首先来看离散型随机变量。考虑随机变量取某一个特定值时包含的信息量的大小。假设随机变量取值为 x，对应的概率为 p(x)。直观来看，取这个值的可能性越小，而它又发生了，则包含的信息量就越大。因此如果定义一个函数 h(x) 来描述随机变量取值为的信息量的大小的话，则 h(x) 应该是 p(x) 的单调减函数。例如，一年之内人类登陆火星，包含的信息量显然比广州明天要下雨大，因为前者的概率明显小于后者。

满足单调递减要求的函数太多了，我们该选择哪个函数呢？接着考虑。假设有两个**相互独立的随机变量**，它们的取值分别为和，取该值的概率为p(x)和p(y)。根据随机变量的独立性，它们的联合概率为：
```mathjax!
$$
p(x,y) = p(x)p(y)
$$
```
由于这两个随机变量是相互独立的，因此它们各自取某一值时包含的信息量应该是两个随机变量分别取这些值的时候包含的信息量之和：
```mathjax!
$$
h(x,y) = h(x) + h(y)
$$
```
这要求 h(x) 能把 p(x) 的乘法转化为加法，在数学上，满足此要求的是对数函数。因此，可以把**信息量**定义为：
```mathjax!
$$
h(x) = -log\ p(x)
$$
```
这个对数的底数是多少并没有太大关系，根据换底公式，最后计算出来的结果就差了一个倍数，信息论中通常以 2 为底，在机器学习中通常以 e 为底，在后面的计算中为了方便起见我们用 10 为底。需要强调的对数函数前面加上了负号，这是因为对数函数是增函数，而我们要求 h(x) 是 p(x) 的减函数。另外，由于`!$0\le p(x)\le 1$`，因此 `!$logp(x)<0$`，加上负号之后刚好可以保证这个信息量为正。

**总结：**
1. “信息量”和“概率”呈反比；
2. 值域：
```mathjax!
$$
\because p(x)\in [0,1],\therefore \frac{1}{p(x)}\in [1,\infty],\therefore log\frac{1}{p(x)}\in [0,\infty]
$$
```

上面只是考虑了随机变量取某一个值时包含的信息量，而随机变量的取值是随机的，有各种可能，那又怎么计算它取所有各种取值时所包含的信息量呢？既然随机变量取值有各种情况，而且取每个值有一个概率，那我们计算它取各个值时的信息量的均值即数学期望即可，这个信息量的均值，就是熵。**信息熵通常用来描述整个随机分布所带来的信息量平均值，更具统计特性。信息熵也叫香农熵，在机器学习中，由于熵的计算是依据样本数据而来，故也叫经验熵。**

对于离散型随机变量，熵定义为：
```mathjax!
$$
H(X) = E_x[-log\ p(x)] = -\sum_i p_i \ log p_i
$$
```
这里约定 `!$P_i = P(x_i)$`。

下面用实际例子来说明离散型随机变量熵的计算。对于下表定义的概率分布：

|   x  |   1  |  2   |   3  |  4   |
| --- | --- | --- | --- | --- |
|  p   |  0.25  |   0.25  |   0.25  |  0.25   |

它的熵为：
```mathjax!
$$
\begin{aligned}
H(X) &= -0.25 \times log 0.25 - 0.25 \times log 0.25 - 0.25 \times log 0.25 - 0.25 \times log 0.25 \\
&=log 4   \\
&= 0.6
\end{aligned}
$$
```
再来看另外一个概率分布：

|   x  |   1  |  2   |   3  |  4   |
| --- | --- | --- | --- | --- |
|  p   |  0.9   |   0.05  |   0.02  |  0.03   |

它的熵为：
```mathjax!
$$
\begin{aligned}
H(X) &= -0.9 \times log 0.9 - 0.05 \times log 0.05 - 0.02 \times log 0.02 - 0.03 \times log 0.03 \\
&=0.041 + 0.065 + 0.034 + 0.046   \\
&= 0.186
\end{aligned}
$$
```
从上面两个结果可以看出一个现象。第一个概率分布非常均匀，随机变量取每个值的概率相等；第二个不均匀，以极大的概率取值为 1，取值为 2-4 的概率非常小。第一个概率分布的熵明显的大于第二个概率分布，即随机变量越均匀（随机），熵越大，反之越小。

下面考虑连续型随机变量。对于连续型随机变量，熵（微分熵）定义为：
```mathjax!
$$
H(X) = - \int_{-\infty}^{+\infty} p(x) log p(x)dx
$$
```
这里将求和换成了广义积分。
 
根据熵的定义，随机变量取各个值的概率相等（均匀分布）时有极大值，在取某一个值的概率为 1，取其他所有值的概率为 0 时有极小值（此时随机变量退化成某一必然事件或者说确定的变量）。下面证明这一结论。

对于离散型随机变量，熵定义的是如下函数：
```mathjax!
$$
H(X) = -\sum_{i=1}^n x_i log x_i
$$
```
其中 `!$x_i$` 为随机变量取第 i 个值的概率。约束条件为：
```mathjax!
$$
\sum_{i=1}^n x_i = 1 \ \ \ \  x_i \ge 0
$$
```
由于对数函数的定义域是非负的，因此可以去掉不等式约束。构造拉格朗日乘子函数：
```mathjax!
$$
L(x,\lambda) = - \sum_{i = 1}^n x_i log x_i + \lambda (\sum_{i=1}^n x_i - 1)
$$
```
对 `!$x_i$` 求偏导并令其为 0，可以得到：
```mathjax!
$$
\frac{\partial L}{\partial x_i} = - log x_i - 1 + \lambda = 0
$$
```
这意味着在极值点处所有的 `!$x_i$`  必须相等。对 `!$\lambda$` 求偏导数并令其为 0，可以得到
```mathjax!
$$
\sum_{i=1}^n x_i = 1
$$
```
因此当 `!$x_i = 1/n$` 时函数取得极值。此时熵的值为：
```mathjax!
$$
H(X) = - \sum_{i = 1}^n \frac 1 n log \frac 1 n = log n
$$
```
进一步的可以证明该值是极大值。熵的二阶偏导数为：
```mathjax!
$$
\frac{\partial^2 H}{\partial x_i^2} = - \frac{1}{x_i} \\
\frac{\partial^2 H}{\partial x_i^2} = 0
$$
```
因此 Hessian 矩阵为：
```mathjax!
$$
\begin{bmatrix}
-1/x_1 & \ldots & 0 \\ \ldots & \ldots & \ldots \\ 0 & \ldots & -1/x_n
\end{bmatrix}
$$
```
由于 `!$x_i > 0$`，该矩阵负定，熵是凹函数，有极大值。因此当 `!$x_i =1/n$` 时熵有极大值。如果定义
```mathjax!
$$
0\ log\ 0 = 0
$$
```
显然它与下面的极限是一致的：
```mathjax!
$$
lim_{x \to 0}\ x log x = 0
$$
```
则当某一个 `!$x_i = 1$`，其他 `!$x_j =0$`, 的时熵有极小值 0：
```mathjax!
$$
H = 0 log 0 + \ldots + 1 log 1 + \ldots + 0 log 0 = 0
$$
```
除此情况之外，只要满足 `!$0 < x_i < 1$`，则 `!$log x_i < 0$`，因此 
```mathjax!
$$
- x_i log x_i > 0
$$
```
上面这些结果说明熵是非负的，当且仅当随机变量取某一值的概率为 1，取其他值的概率为 0 时熵有极小值 0。此时随机变量退化成普通的变量，取值固定。而当随机变量取所有值的概率相等时即均匀分布时熵有极大值。故熵的范围为：
```mathjax!
$$
0 \le H(X) \le log\ n
$$
```
下面举例说明熵在机器学习中的应用，以决策树为例。用于对分类问题，决策树在训练每个非叶子节点时要寻找最佳分裂，将样本进行划分成尽可能纯的子集。此时熵的作用是度量数据集的“纯度”值。样本集 D 的熵不纯度定义为：
```mathjax!
$$
E(D) = -\sum_i p_i log_2 p_i
$$
```
当样本只属于某一类时熵有最小值，当样本均匀的分布于所有类中时熵有最大值。找到一个分裂让熵最小化，它就是最佳分裂。


#### 2、条件熵
条件熵的定义为：在 X 给定条件下，Y 的条件概率分布的熵对 X 的数学期望。具体公式为：
```mathjax!
$$
\begin{aligned} H(Y|X) &= E_{x \sim p}[H(Y|X = x)] \\  &= \sum^n_{i = 1}p(x_i)H(Y|X = x) \\ &= - \sum^n_{i = 1}p(x_i) \sum^m_{j = 1}p(y_j|x_i)log p(y_j|x_i) \\ &= - \sum^n_{i = 1} \sum^m_{j = 1}p(x_i,y_j)log p(y_j|x_i) \\ &= -(\sum^n_{i = 1} \sum^m_{j = 1}p(x_i,y_j)log p(x_i,y_j) - \sum^n_{i = 1} \sum^m_{j = 1}p(x_i,y_j)log p(x_i)) \\ &= -(\sum^n_{i = 1} \sum^m_{j = 1}p(x_i,y_j)log p(x_i,y_j) - \sum^n_{i = 1}log p(x_i) \sum^m_{j = 1}p(x_i,y_j)) \\ &= -(\sum^n_{i = 1} \sum^m_{j = 1}p(x_i,y_j)log p(x_i,y_j) - \sum^n_{i = 1}p(x_i)log p(x_i)) \\ &= H(X,Y) - H(X)
\end{aligned}
$$
```
同理可得：`!$H(X|Y) = H(X,Y) - H(Y)$`。

理解条件熵可以使用决策树进行特征选择的例子：我们期望选择的特征要能将数据的标签尽可能分得比较“纯”一些，特征将数据的标签分得“纯”，则熵就小，信息增益就大。

#### 3、联合熵
两个变量  X 和 Y  的联合熵的表达式：
```mathjax!
$$
H(X,Y) = -\sum^n_{i = 1}p(x_i - y_i)log p(x_i,y_i)
$$
```

#### 4、互信息
互信息在信息论和机器学习中非常重要，其可以评价两个分布之间的距离，这主要归因于其对称性，假设互信息不具备对称性，那么就不能作为距离度量，例如相对熵，由于不满足对称性，故通常说相对熵是评价分布的相似程度，而不会说距离。互信息的定义为：一个随机变量由于已知另一个随机变量而减少的不确定性，或者说从贝叶斯角度考虑，由于新的观测数据y到来而导致x分布的不确定性下降程度。公式如下：
```mathjax!
$$
\begin{aligned}
I(X,Y) &= H(X) - H(X|Y) = H(Y) - H(Y|X) \\ &= H(X) + H(Y) - H(X,Y) \\ &= H(X,Y) - H(Y|X) - H(X|Y)
\end{aligned}
$$
```

根据信息熵、条件熵的定义式，可以计算信息熵与条件熵之差：
```mathjax!
$$
\begin{aligned}
H(X) - H(X|Y) &= -\sum^n_{i = 1}p(x_i)log p(x_i) - \sum^m_{j = 1}p(y_j)H(X|Y = y) \\ &= -\sum^n_{i = 1}\Big(\sum^m_{j = 1}p(x_i,y_j)\Big)log p(x_i) + \sum^m_{j = 1}p(y_j)\sum^n_{i = 1}p(x_i|y_j)log p(x_i|y_j) \\ &= -\sum^n_{i = 1}\sum^m_{j = 1}p(x_i,y_j)log p(x_i) + \sum^m_{j = 1}\sum^n_{i = 1}p(x_i,y_j)log p(x_i|y_j) \\ &= \sum^m_{j = 1}\sum^n_{i = 1}p(x_i,y_j)log \frac{p(x_i|y_j)}{p(x_i)} \\ &= \sum^m_{j = 1}\sum^n_{i = 1}p(x_i,y_j)log \frac{p(x_i,y_j)}{p(x_i)p(y_j)}
\end{aligned}
$$
```
同理：
```mathjax!
$$
\begin{aligned}
H(Y) - H(Y|X) &= -\sum^m_{j = 1}p(y_j)log p(y_j) - \sum^n_{i = 1}p(x_i)H(Y|X = x) \\ &= -\sum^m_{j = 1}\Big(\sum^n_{i = 1}p(x_i,y_j)\Big)log p(y_j) + \sum^n_{i = 1}p(x_i)\sum^m_{j = 1}p(y_j|x_i)log p(y_i|x_i) \\ &= -\sum^m_{j = 1}\sum^n_{i = 1}p(x_i,y_j)log p(y_j) + \sum^n_{i = 1}\sum^m_{j = 1}p(y_j,x_i)log p(y_j|x_i) \\ &= \sum^n_{i = 1}\sum^m_{j = 1}p(x_i,y_j)log \frac{p(y_j|x_i)}{p(y_j)} \\ &= \sum^n_{i = 1}\sum^m_{j = 1}p(x_i,y_j)log \frac{p(x_i,y_j)}{p(x_i)p(y_j)}
\end{aligned}
$$
```
因此：`!$ H(X) - H(X|Y) = H(Y) - H(Y|X)$`

从公式中可以看出互信息是满足对称性的，其在特性选择、分布的距离评估中应用非常广泛，请务必掌握。其实互信息和相对熵也存在联系，如果说相对熵不能作为距离度量，是因为其非对称性，那么互信息的出现正好弥补了该缺陷，使得我们可以计算任意两个随机变量之间的距离，或者说两个随机变量分布之间的相关性、独立性。
```mathjax!
$$
I(X,Y) = KL\Big(p(x,y) || p(x)p(y)\Big)
$$
```
互信息也是大于等于0的，当且仅当x与y相互独立时候取等号。

##### 4.1、信息增益
信息增益是决策树ID3算法在进行特征切割时使用的划分准则，其物理意义和互信息完全相同，并且公式也是完全相同。其公式如下：
```mathjax!
$$
g(D,A) = H(D) - H(D|A)
$$
```

其中D表示数据集，A表示特征，信息增益表示得到A的信息而使得类X的不确定度下降的程度，在ID3中，需要选择一个A使得信息增益最大，这样可以使得分类系统进行快速决策。

需要注意的是：在数值上，信息增益和互信息完全相同，但意义不一样，需要区分，当我们说互信息时候，两个随机变量的地位是相同的，可以认为是纯数学工具，不考虑物理意义，当我们说信息增益时候，是把一个变量看成是减少另一个变量不确定度的手段。

##### 4.2、信息增益率
信息增益率是决策树C4.5算法引入的划分特征准则，其主要是克服信息增益存在的在某种特征上分类特征细，但实际上无意义取值时候导致的决策树划分特征失误的问题。例如假设有一列特征是身份证ID，每个人的都不一样，其信息增益肯定是最大的，但是对于一个情感分类系统来说，这个特征是没有意义的，此时如果采用ID3算法就会出现失误，而C4.5正好克服了该问题。其公式如下：
```mathjax!
$$
g_r(D,A) = g(D,A) / H(A)
$$
```

##### 4.3、基尼系数
基尼系数是决策树CART算法引入的划分特征准则，其提出的目的不是为了克服上面算法存在的问题，而主要考虑的是计算快速性、高效性，这种性质使得CART二叉树的生成非常高效。其公式如下：
```mathjax!
$$
Cini(p) = \sum^m_{i = 1}p_i(1 - p_i) = 1 - \sum^m_{i = 1}p_i^2 = 1 - \sum^m_{i = 1}(\frac{|C_k|}{|D|})^2
$$
```
可以看出，基尼系数越小，表示选择该特征后熵下降最快，对分类模型效果更好，其和信息增益和信息增益率的选择指标是相反的。基尼系数主要是度量数据划分对训练数据集D的不纯度大小，基尼系数越小，表明样本的纯度越高。

这里还存在一个问题，这个公式显得非常突兀，感觉突然就出来了，没有那种从前人算法中改进而来的感觉？其实为啥说基尼系数计算速度快呢，因为基尼系数实际上是信息熵的一阶进似，作用等价于信息熵，只不过是简化版本。根据泰勒级数公式，将 `!$f(x) = -ln(x)$` 在 `!$x=1$` 处展开，忽略高阶无穷小，其可以等价为  `!$f(x) = 1 - x$` ，所以可以很容易得到上述定义。

#### 5、相对熵
相对熵是一个较高端的存在，其作用和交叉熵差不多。相对熵经常也叫做KL散度（Kullback-Leibler (KL) divergence），如果我们对于同一个随机变量  X 有两个单独的概率分布 `!$p(x)$`  和 `!$q(x)$`  在贝叶斯推理中，`!$D_{KL}(p||q)$` 衡量这两个分布的差异。差异越大则相对熵越大，差异越小则相对熵越小。首先给出其公式：
```mathjax!
$$
\begin{aligned}
D_{KL}(p||q) &= E_{x\sim p}\Big[log\frac{p(x)}{q(x)}\Big] = - E_{x\sim p}\Big[log\frac{q(x)}{p(x)}\Big] \\ &= -\sum^n_{i = 1}p(x)log\frac{q(x)}{p(x)} = H(p,q) - H(p)
\end{aligned}
$$
```
如何记忆：如果用 `!$p(x)$` 来描述样本，那么就非常完美（因为 `!$p(x)$` 认为是真实的情况）。而用 `!$q(x)$` 来描述样本，虽然可以大致描述，但是不是那么的完美，信息量不足，需要额外的一些“信息增量”才能达到和 `!$p(x)$` 一样完美的描述。如果我们的 `!$q(x)$` 通过反复训练，也能完美的描述样本，那么就不再需要额外的“信息增量”，`!$q(x)$` 等价于 `!$p(x)$` 。即`!$p(x)$` 和 `!$q(x)$` 的分布完全一致的时候，KL 散度的值等于  `!$0$`。

相对熵较交叉熵有更多的优异性质，主要为：
1. 当p分布和q分布相等时候，KL散度值为0，这是一个非常好的性质；
2. 可以证明是非负的；
3. 非对称的，通过公式可以看出，KL散度是衡量两个分布的不相似性，不相似性越大，则值越大，当完全相同时，取值为0。

简单对比交叉熵和相对熵，可以发现仅仅差了一个 `!$H(p)$`，如果从优化角度来看，p是真实分布，是固定值，最小化KL散度情况下，`!$H(p)$` 可以省略，此时交叉熵等价于KL散度。

下面讨论一个比较现实且非常重要的问题：既然相对熵和交叉熵表示的含义一样，为啥需要两个？在机器学习中何时使用相对熵，何时使用交叉熵？要彻底说清这个问题，难度很大，这里我仅仅从我知道的方面讲讲。首先需要明确：在最优化问题中，最小化相对熵等价于最小化交叉熵；相对熵和交叉熵的定义其实都可以从最大似然估计得到，下面进行详细推导：以某个生成模型算法为例，假设是生成对抗网络GAN，其实只要是生成模型，都满足以下推导。若给定一个样本数据的真实分布 `!$P_{data}(x)$` 和生成的数据分布 `!$P_G(x;\theta)$` ，那么生成模型希望能找到一组参数 `!$\theta$` 使分布`!$P_G(x;\theta)$` 和 `!$P_{data}(x)$` 之间的距离最短，也就是找到一组生成器参数而使得生成器能生成十分逼真的分布。现在从真实分布 `!$P_{data}(x)$` 中抽取 `!$m$` 个真实样本 `!$x^1,x^2,\ldots,x^m$` ,对于每一个真实样本，我们可以计算 `!$P_G(x^i;\theta)$` ，即在由 `!$\theta$` 确定的生成分布中， `!$x^i$` 样本所出现的概率。因此，我们可以构建似然函数：
```mathjax!
$$
L = \prod^m_{i = 1}P_G(x^i;\theta)
$$
```
最大化似然函数，即可求得最优参数 `!$\theta^*$`：
```mathjax!
$$
\theta^* = arg\ \underbrace{max}_{\theta}\prod^m_{i = 1}P_G(x^i;\theta)
$$
```
转换为对数似然函数：
```mathjax!
$$
\begin{aligned}
\theta^* &= arg\ \underbrace{max}_{\theta}\prod^m_{i = 1}P_G(x^i;\theta) \\ &= arg\ \underbrace{max}_{\theta}\sum^m_{i = 1}log P_G(x^i;\theta)
\end{aligned}
$$
```
由于是求最大值，故整体乘上常数对结果没有影响,这里是逐点乘上一个常数，所以不能取等于号，但是因为在取得最大值时候 `!$P_G(x;\theta^*)$` 和 `!$P_{data}(x)$` 肯定是相似的，并且肯定大于0，所以依然可以认为是近视相等的：
```mathjax!
$$
\begin{aligned}
&\approx arg\ \underbrace{max}_{\theta}\sum^m_{i = 1}P_{data}(x^i)log P_G(x^i;\theta) \\ &= arg\ \underbrace{max}_{\theta}\ E_{x\sim p_{data}}\Big[log P_G(x^i;\theta)\Big]
\end{aligned}
$$
```
上面的公式正好是交叉熵的定义式。然后我们再该基础上减掉一个常数：
```mathjax!
$$
\begin{aligned}
\theta^* &= arg\ \underbrace{max}_{\theta}\ \{E_{x\sim p_{data}}\Big[log P_G(x^i;\theta)\Big] - E_{x\sim p_{data}}\Big[log P_{data}(x^i)\Big]\} \\ &= arg\ \underbrace{max}_{\theta} \int_x P_{data} log\frac{P_G(\theta)}{P_{data}}dx \\ &= arg\ \underbrace{min}_{\theta} \int_x P_{data} log\frac{P_{data}}{P_G(\theta)}dx \\ &= arg\ \underbrace{min}_{\theta}\ E_{x\sim P_{data}}\Big[log\frac{P_{data}}{P_G(\theta)}\Big] \\ &= arg\ \underbrace{min}_{\theta}\ KL(P_{data}(x)||P_G(x;\theta))
\end{aligned}
$$
```

通过以上各公式可以得出以下结论：**最大化似然函数，等价于最小化负对数似然，等价于最小化交叉熵，等价于最小化KL散度。**

推导了半天，依然没有回答上面的问题。学过机器学习的同学都知道：交叉熵大量应用在Sigmoid函数和Softmax函数中，最典型的算法应该就是神经网络和逻辑回归吧，而相对熵大量应用在生成模型中，例如GAN、EM、贝叶斯学习和变分推导中。从这里我们可以看出一些端倪，如果想通过算法对样本数据进行概率分布建模，那么通常都是使用相对熵，因为我们需要明确的知道生成的分布和真实分布的差距，最好的KL散度值应该是0；而在判别模型中，仅仅只需要评估损失函数的下降值即可，交叉熵可以满足要求，其计算量比KL散度小。在数学之美书中，有这样几句话：交叉熵，其用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小，相对熵，其用来衡量两个取值为正的函数或概率分布之间的差异。但是我觉得依然看不出区别。


#### 6、交叉熵
交叉熵的定义与熵类似，不同的是定义在两个概率分布而不是一个概率分布之上。对于离散型随机变量，交叉熵定义为：
```mathjax!
$$
H(p,q) = -\sum_x p(x) log\ q(x)
$$
```
其中x为离散型随机变量，`!$p(x)$` 和 `!$q(x)$` 是它的两个概率分布。交叉熵衡量了两个概率分布的差异。其值越大，两个概率分布相差越大；其值越小，则两个概率分布的差异越小。
 
下面通过实际例子来说明交叉熵的计算。对于下表的两个概率分布：

|   x  |  1  |  2   |  3   |  4   |
| --- | --- | --- | --- | --- |
|   p  |  0.4   |  0.4   |  0.1   |  0.1   |
|  q   |  0.4   |  0.4   |  0.1   |  0.1   |

其交叉熵为：
```mathjax!
$$
H(p,q) = -0.4 \times log 0.4 -0.4 \times log 0.4 -0.1 \times log 0.1 -0.1 \times log 0.1 = 0.518
$$
```

对于下表的两个概率分布：
|   x  |  1  |  2   |  3   |  4   |
| --- | --- | --- | --- | --- |
|   p  |  0.4   |  0.4   |  0.1   |  0.1   |
|  q   |  0.1   |  0.1   |  0.4   |  0.4   |

其交叉熵为：
```mathjax!
$$
H(p,q) = -0.1 \times log 0.4 -0.1 \times log 0.4 -0.4 \times log 0.1 -0.4 \times log 0.1 = 0.88
$$
```

第一个表格中两个概率分布完全相等，第二个则差异很大。第二个的熵比第一个大。
 
对于连续型概率分布，交叉熵定义为：
```mathjax!
$$
\int_x p(x)log q(x)dx = E_p[-log q]
$$
```

**如果两个概率分布完全相等，则交叉熵退化成熵。**
 
可以证明，当两个分布相等的时候，交叉熵有极小值。假设第一个概率分布固定即 `!$p(x)$` 为常数，此时交叉熵为如下形式的函数：
```mathjax!
$$
H(x) = -\sum^n_{i = 1}a_i log x_i
$$
```
约束条件为：
```mathjax!
$$
\sum^n_{i = 1}x_i = 1
$$
```
构造拉格朗日乘子函数：
```mathjax!
$$
L(x,\lambda) = -\sum^n_{i = 1}a_i log x_i + \lambda(\sum^n_{i = 1}x_i - 1)
$$
```
对所有变量求偏导数，并令偏导数为0，有：
```mathjax!
$$
\begin{aligned}
-\frac{a_i}{x_i} + \lambda = 0 \\
\sum^n_{i = 1}x_i = 1 \\
\sum^n_{i = 1}a_i = 1
\end{aligned}
$$
```
最后可以解得：`!$\lambda = 1,\ \ x_i = a_i$`。
交叉熵函数的Hessian矩阵为：
```mathjax!
$$
\begin{bmatrix} a_1 / x_1^2 & \ldots & 0 \\ \ldots & \ldots & \ldots \\ 0 & \ldots & a_n / x_n^2 \end{bmatrix}
$$
```
该矩阵正定，因此交叉熵损失函数是凸函数，上面的极值点是极小值点。

##### 6.1、用于logistic回归
交叉熵在机器学习中用得更多，通常用于充当目标函数，以最小化两个概率分布的差异。下面介绍交叉熵在logistic回归中的应用。logistic的预测函数为：
```mathjax!
$$
h(x) = \frac{1}{1 + exp(-w^Tx)}
$$
```
样本属于正样本的概率为
```mathjax!
$$
p(y = 1|x) = h(x)
$$
```
属于负样本的概率为
```mathjax!
$$
p(y = 0|x) = 1 - h(x)
$$
```
其中y为类别标签，取值为1或者0，分别对应正负样本。

训练样本集为`!$(x_i,y_i)，i=1,...,l$`, `!$x_i$` 为特征向量，`!$y_i$` 为类别标签，取值为1或0。给定w参数和样本特征向量x，样本属于每个类的概率可以统一写成如下形式：
```mathjax!
$$
$$
```

##### 6.2、用于softmax回归
softmax回归是logistic回归的推广，用于解决多分类问题。给定 l 个训练样本 `!$(x_i,y_i)$`，其中 `!$x_i$` 为n维特征向量，`!$y_i$` 为类别标签，取值为 1 到 k 之间的整数。softmax回归估计一个样本属于每一类的概率
```mathjax!
$$
$$
```

##### 6.3、用于神经网络
softmax回归经常被用作神经网络的最后一层，完成多分类任务，训练时采用的损失函数一般为交叉熵。在神经网络的早期，更多的使用了欧氏距离损失函数，后来对分类任务交叉熵使用的更多。对于分类问题，交叉熵一般比欧氏距离有更好的效果，可以收敛到更好的局部最优解，具体的可以参考文献[4]。此时训练样本的类别标签向量同样为one-hot编码形式，是类别取每个值的概率，神经网络输出的概率估计向量要和真实的样本标签向量接近。具体的推导和证明可以阅读《机器学习与应用》一书，与softmax回归类似，在这里不再详细讲述。除了上面列出的这些机器学习算法之外，交叉熵在AdaBoost算法、梯度提升算法中也有应用，感兴趣的读者可以阅读《机器学习与应用》。

#### 7、总结

自信息是衡量随机变量中的某个事件发生时所带来的信息量的多少，越是不可能发生的事情发生了，那么自信息就越大；信息熵是衡量随机变量分布的混乱程度，是随机分布各事件发生的自信息的期望值，随机分布越宽广，则熵越大，越混乱；信息熵推广到多维领域，则可得到联合信息熵；在某些先验条件下，自然引出条件熵，其表示在X给定条件下，Y的条件概率分布熵对X的数学期望，没有啥特别的含义，是一个非常自然的概念；前面的熵都是针对一个随机变量的，而交叉熵、相对熵和互信息可以衡量两个随机变量之间的关系，三者作用几乎相同，只是应用范围和领域不同。交叉熵一般用在神经网络和逻辑回归中作为损失函数，相对熵一般用在生成模型中用于评估生成的分布和真实分布的差距，而互信息是纯数学的概念，作为一种评估两个分布之间相似性的数学工具，其三者的关系是：最大化似然函数，等价于最小化负对数似然，等价于最小化交叉熵，等价于最小化KL散度，互信息相对于相对熵区别就是互信息满足对称性；作为熵的典型机器学习算法-决策树，广泛应用了熵进行特征划分，常用的有信息增益、信息增益率和基尼系数。

![信息熵、联合熵、条件熵、互信息的关系](./images/熵.jpg)

信息熵：左边的椭圆代表 `!$H(X)$` ，右边的椭圆代表  `!$H(Y)$`。
互信息（信息增益）：是信息熵的交集，即中间重合的部分就是  `!$I(X,Y)$`。
联合熵：是信息熵的并集，两个椭圆的并就是 `!$H(X,Y)$`。
条件熵：是差集。左边的椭圆去掉重合部分就是 `!$H(X|Y)$`，右边的椭圆去掉重合部分就是 `!$H(Y|X)$`。

还可以看出：
```mathjax!
$$
\begin{gathered}
I(X,Y) = H(X|Y)H(Y|X) - H(X,Y) \\ H(Y|X) = H(X,Y) - H(X) \ \ \ \ \  H(X|Y) = H(X,Y) - H(Y) \\ H(X|Y) \le H(X) \ \ \ \ \  H(Y|X) \le H(Y)
\end{gathered}
$$
```


**参考：**
[1]. https://zhuanlan.zhihu.com/p/35423404

[2]. https://mp.weixin.qq.com/s/wQbbFJPtTFrSpO5Qbe7XKA

[3]. https://mp.weixin.qq.com/s/O1cc2W8M3thJoKN6-uRmEQ

[4]. P Golik, P Doetsch, H Ney. Cross-Entropy vs. Squared Error Training: a Theoretical and Experimental Comparison. 2013