---
title: Softmax函数
tags: softmax,分类
grammar_cjkRuby: true
---

#### 一、定义
在数学，尤其是概率论和相关领域中，Softmax函数（或称归一化指数函数）是逻辑函数的一种推广。它能将一个含任意实数的 N 维的向量 z “压缩”到另一个 N 维实向量 `!$\sigma(z)$` 中，使得每一个元素的范围都在 (0,1) 之间，并且所有元素的和为 1。

Softmax函数的输入是 N 维的随机真值向量，输出是另一个 N 维的真值向量。即映射：`!$S(a) = \Bbb {R}^N \rightarrow \Bbb {R}^N$`:
```mathjax!
$$
S(a) : \begin{bmatrix} a_1 \\ a_2 \\ \cdots \\ a_N \end{bmatrix} \rightarrow \begin{bmatrix} S_1 \\ S_2 \\ \cdots \\ S_N \end{bmatrix}
$$
```

###### 概率解释
softmax的性质(所有输出的值范围是(0, 1)且和为 1 )使其在机器学习的概率解释中广泛使用。尤其是在多类别分类任务中，我们总是给输出结果对应的类别附上一个概率，即如果我们的输出类别有 N 种，我们就输出一个 N 维的概率向量且和为 1。每一维的值对应一种类别的概率。我们可以将softmax解释如下：
```mathjax!
$$
S_j = P(y = j | a)
$$
```
其中，`!$y$` 是输出的 N 个类别中的某个(取值为 `!$1...N$`)。`!$a$` 是任意一个 N 维向量。最常见的例子是多类别的逻杰斯谛回归，输入的向量 `!$x$` 乘以一个权重矩阵 `!$W$`，且该结果输入softmax函数以产生概率。事实证明，从概率的角度来看，softmax对于模型参数的最大似然估计是最优的。**(在代价函数的笔记中会详细说明)**
 
#### 二、公式
Softmax是一种形如下式的函数：
```mathjax!
$$
P(i) = \frac{exp(\theta_i^T x)}{\sum_{k = 1}^K exp(\theta_k^T x)}
$$
```
其中 `!$\theta_i $` 和 `!$ x $` 是列向量，`!$\theta_i^T x$` 可能被换成关于 `!$ x $` 的函数 `!$f_i(x)$`。

显然 `!$P(i)$` 总是正的(因为指数)；因为所有的 `!$P(i)$` 的和为 1，使得 `!$P(i)$` 的范围在 `!$[0,1]$` 之间。在回归和分类问题中，函数的输入是从 `!$K$` 个不同的线性函数得到的结果，而  `!$P(i)$` 表示样本向量 `!$x$` 属于第  `!$i$` 个分类的概率，<span style="font-size:100%;color:red">通常 `!$\theta$` 是待求参数，通过寻找使得 `!$P(i)$` 最大的 `!$\theta_i$` 作为最佳参数。</span>

和logistic函数一样，softmax函数加入了 `!$e$` 的幂函数正是为了两极化：正样本的结果将趋近于 1，而负样本的结果趋近于0。这样为多类别分类提供了方便（可以把  `!$P(i)$` 看作是样本属于类别 `!$i$` 的概率）。可以说，Softmax函数是logistic函数的一种泛化。
 