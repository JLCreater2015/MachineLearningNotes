---
title: 正则化 
tags: 深度学习,正则化
grammar_cjkRuby: true
---

## 1、模型过拟合

#### 1.1、什么是模型的过拟合

训练的模型过拟合，根据方差+偏差的分解，则说明“方差”很大，直观的含义就是，模型的稳定性不强，表现在某一个特征输入数据“稍有波动”，模型的效果会变差。因为在测试集上面的很多数据都是没有见过的，相比于训练数据，难免会有差别，故而如果用一个过拟合的模型，在测试集上面的表现自然不好。就是过度学习训练集的特征导致在训练集上表现好而在测试机上表现差。

我们称这样的模型“太过复杂”了（注意是引号），复杂的体现在于，模型中求出的参数在训练的时候为了“迎合”误差的减少，很多参数很大（过分强调一些特征），很多参数有很小（一些微小的特征），这样就会导致模型会有一种“偏爱”。自然，因为模型的这点偏爱，导致的结果就是，测试集的数据稍有波动，表现就不好了，即所谓的过拟合。与其说是“复杂”，倒不如说是“畸形”更恰当。

#### 1.2、过拟合的解决办法

自然，我们希望遇到更“简单”的模型，“简单”的方式有以下几种：
1. 获取更多的样本
2. 丢弃一部分特征，增强模型的容错能力（比如PCA降维，神经网络的dropout机制）
3. 不丢其特征，保留所有的特征，但是减少参数的大小（magnitude）。确保所有的特征对于预测都有所贡献，而不是“偏爱”某几种特征。

当每一个特征 `!$X_i$` 对 `!$y$` 都有贡献的时候，这样的模型是比较健康稳定的，可以良好地工作，这就是正则化的目的，可以用来解决过拟合。

<p style="color:red">正则化策略：以增大训练误差为代价，来减少测试误差（如果在训练误差上很小，可能出现过拟合的情况）；</p>

这里有一个理解层面的不同：

1. 有的将 L1,L2 称之为正则化，因为这是最普遍的用法，此正则化减少模型的“复杂度”是通过让模型的参数不那么“畸形”实现的，并没有减少参数的个数。
2. 有的将dropout技术也称之为正则化，因为它通过丢弃参数，也达到了减轻模型“复杂度”的目的。
3. 有的甚至更加笼统，只要是能够较轻模型“过拟合”的技术，全部称之为“正则化”毕竟正则化是以增大偏差为代价，减少模型的方差。所以只要是能够防止过拟合的手段，都可以称之为正则化，包括L1,L2（范数约束）,dropout、 drop connect、早停法、数据增强与扩充等等都统称正则法。


## 2、L1、L2正则化

#### 2.1、数学基础

##### 2.1.1. 范数
范数是衡量某个向量空间（或矩阵）中的每个向量的长度或大小。范数的一般化定义：
对实数 `!$p\ge 1$`， 范数定义如下：
```mathjax!
$$
||x||_p := (\sum_{i = 1}^n |x_i|^p)^{\frac{1}{p}}
$$
```

- **L1范数**
当p=1时，是L1范数，其表示某个向量中所有元素绝对值的和。
- **L2范数**
当p=2时，是L2范数， 表示某个向量中所有元素平方和再开根， 也就是欧几里得距离公式。

##### 2.1.2. 拉普拉斯分布
如果随机变量的概率密度函数分布为:
```mathjax!
$$
f(x|\mu,b) = \frac{1}{2b} exp(- \frac{|x-\mu|}{b}) \\
= \frac{1}{2b} \begin{cases} exp(- \frac{\mu - x}{b}), & \text{if } x < \mu \\ exp(- \frac{x - \mu}{b}), & \text{if } x\ge \mu \end{cases}
$$
```

那么它就是拉普拉斯分布。其中，`!$\mu$` 是数学期望，`!$b > 0$` 是振幅。如果 `!$\mu = 0$`，那么，正半部分恰好是尺度为 `!$1/2$` 的指数分布。

![拉普拉斯分布的概率密度函数](./images/1571376013617.png)

##### 2.1.3. 高斯分布
又叫正态分布，若随机变量 X 服从一个数学期望为 `!$\mu$`、标准方差为 `!$\sigma^2$` 的高斯分布，记为：`!$X∼N(\mu,\sigma^2)$`，其概率密度函数为:
```mathjax!
$$
f(x) = \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{(x - \mu)^2}{2 \sigma^2}}
$$
```

其概率密度函数为正态分布的期望值 `!$\mu$` 决定了其位置，其标准差 `!$\sigma$` 决定了分布的幅度。当 `!$\mu = 0,\sigma = 1$` 时的正态分布是标准正态分布。

![高斯分布的概率密度函数](./images/1571376253065.png)

#### 2.2、正则化

Regularization：正则化是机器学习中常用的一种技术，主要目的是：**控制模型复杂度，减小过拟合。**

最基本的正则化方法是在原目标（代价）函数的基础上加上一个正则项，对复杂度高的模型进行“惩罚”，数学表达式为：
```mathjax!
$$
\tilde{J}(w;X,y) = J(w;X,y) + \alpha \Omega(w)  \tag{2.1}
$$
```

其中:

`!$X$`：训练样本

`!$y$`：训练样本对应的标签

`!$w$`：权重系数

`!$J()$`：目标函数

`!$\Omega (w)$`：为惩罚项，可以理解为模型“规模”的一种度量

`!$\alpha$`：控制正则化的强弱

不同的 `!$\Omega$` 对不同的权重w有着不同的“偏好”，因而会产生不同的效果

最常用的 `!$\Omega$` 有两种：L1范数和L2范数，带入之后得到：
```mathjax!
$$
l_1 : \Omega(w) = ||w||_1 = \sum_i |w_i| \\
l_2 : \Omega(w) = ||w||_2^2 = \sum_i w_i^2
$$
```

##### 2.2.1、L1/L2正则化来源推导

###### 2.2.1.1、正则化理解之基于约束条件的最优化

对于模型权重系数 `!$w$` 的求解释通过最小化目标函数实现的，也就是求解：
```mathjax!
$$
\underset{w}{min} J(w;X,y)  \tag{2.2}
$$
```
首先，模型的复杂度可以用VC来衡量。通常情况下，模型VC维与系数 `!$w$` 的个数成线性关系：即：**w数量越多，VC越大，模型越复杂**

为了限制模型的复杂度，我们要降低VC，自然的思路就是降低w的数量，即：让 `!$w$` 向量中的一些元素为0或者说限制 `!$w$` 中非零元素的个数。我们可以在原优化问题上加入一些优化条件：
```mathjax!
$$
\underset{w}{min} J(w;X,y) \ \ \ \ \   s.t.||w||_0 \le C   \tag{2.3}
$$
```
其中约束条件中的 `!$||w||_0$` 是指L0范数，表示的是向量 w 中非零元素的个数，让非零元素的个数小于某一个C，就能有效地控制模型中的非零元素的个数，但是这是一个NP问题，不好解，于是我们需要做一定的“松弛”。为了达到我们想要的效果（权重向量w中尽可能少的非零项），我们不再严格要求某些权重 `!$w$` 为0，而是要求权重 w 向量中某些维度的非零参数尽可能接近于0，尽可能的小，这里我们可以使用L1、L2范数来代替L0范数，即：
```mathjax!
$$
\underset{w}{min} J(w;X,y) \ \ \ \ \   s.t.||w||_1 \le C \\
\text{或 } \ \ \ \underset{w}{min} J(w;X,y) \ \ \ \ \   s.t.||w||_2 \le C  \tag{2.4}
$$
```
使用 L2 范数时，为方便后续处理，可对 `1$||w||_2$` 进行平方，此时只需调整 C 的取值即可。利用拉格朗日算子法，我们可将上述带约束条件的最优化问题转换为不带约束项的优化问题，构造拉格朗日函数：
```mathjax!
$$
L(w,\alpha) = J(w;X,y) + \alpha(||w||_1 - C) \\
\text{或 } \ \ \ L(w,\alpha) = J(w;X,y) + \alpha(||w||_2^2 - C)     \tag{2.5}
$$
```
其中`!$\alpha > 0$` ，我们假设 `!$\alpha$` 的最优解为 `!$\alpha^*$`，则对拉格朗日函数求最小化等价于：
```mathjax!
$$
\underset{w}{min}J(w;X,y) + \alpha^* ||w||_1 \\
\text{或 } \ \ \ \underset{w}{min} J(w;X,y) + \alpha^* ||w||_2^2     \tag{2.6}
$$
```

可以看出，上式与 `!$\underset{w}{min}\tilde{J}(w;X,y)$` 等价。

故此，我们得到对 L1 、 L2 正则化的第一种理解：

- L1: 正则化等价于在原优化目标函数中增加约束条件 `!$||w||_1 \le C$`
- L2: 正则化等价于在原优化目标函数中增加约束条件 `!$||w||_2^2 \le C$`

###### 2.2.1.2、正则化理解之最大后验概率估计

在最大似然估计中，是假设权重 `!$w$` 是未知的参数，从而求得对数似然函数：
```mathjax!
$$
l(w) = log[P(y|X;w)] = log\Big[\prod_i P(y^i | x^i;w)\Big]   \tag{2.7}
$$
```
通过假设 `!$y^i$` 的概率分布，即可得到不同的模型。例如若假设 `!$y^i \sim N(w^Tx^i,\sigma^2)$` 的高斯分布，则有：
```mathjax!
$$
\begin{aligned} 
l(w) &= log\Bigg[\prod_i \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y^i - w^Tx^i)^2}{2\sigma^2}}\Bigg] \\ &= -\frac{1}{2\sigma^2} \sum_i(y^i - w^Tx^i)^2 + C 
\end{aligned}  \tag{2.8}
$$
```
式中 `!$C$` 为常数项，由于常数项和系数项不影响 `!$max\ l(w)$` 的解，因而可令 `!$J(w;X,y) = -l(w)$` 即可得到线性回归的代价函数。

在最大后验概率估计中，则将权重 w 看作随机变量，也具有某种分布，从而有：
```mathjax!
$$
P(w|X,y) = \frac{P(w,X,y)}{P(X,y)} = \frac{P(X,y|w)P(w)}{P(X,y)} \propto P(y|X,w)P(w)  \tag{2.9}
$$
```
同样取对数有：
```mathjax!
$$
MAP = log P(w,X,y)P(w) = log P(y|X,w) + log P(w)   \tag{2.10}
$$
```
可以看出后验概率函数为在似然函数的基础上增加了一项 `!$log P(w)$`。`!$P(w)$` 的意义是对权重系数 `!$w$` 的概率分布的先验假设，在收集到训练样本 `!$\{X,y\}$` 后，则可根据 w 在  `!$\{X,y\}$` 下的后验概率对 `!$w$` 进行修正，从而做出对 `!$w$` 更好地估计。

若假设 `!$w_j$` 的先验分布为0均值的高斯分布，即 `!$w_j \sim N(0,\sigma^2)$`，则有：
```mathjax!
$$
\begin{aligned} 
log P(w) &= log \prod_j P(w_j) \\
&= log \prod_j \Bigg[\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(w_j)^2}{2\sigma^2}}\Bigg] \\
&= - \frac{1}{2\sigma^2}\sum_j w_j^2 + C' 
\end{aligned}  \tag{2.11}
$$
```
可以看到，在高斯分布下 `!$log P(w)$` 的效果等价于在代价函数中增加 L2 正则项。

若假设 `!$w_j$` 服从均值为 0、参数为 `!$a$` 的拉普拉斯分布，即：
```mathjax!
$$
P(w_j) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{|w_j|}{2\sigma^2}}  \tag{2.12}
$$
```
则有：
```mathjax!
$$
log P(w) = log \prod_j \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{|w_j|}{2\sigma^2}} = - \frac{1}{a} \sum_j|w_j| + C'  \tag{2.13}
$$
```
可以看到，在拉普拉斯分布下 `!$log P(w)$` 的效果等价于在代价函数中增加 L1 正则项。

故此，我们得到对于 L1 、L2 正则化的第二种理解：

- L1: 正则化可通过假设权重 `!$w$` 的先验分布为拉普拉斯分布，由最大后验概率估计导出；
- L2: 正则化可通过假设权重 `!$w$` 的先验分布为高斯分布，由最大后验概率估计导出。

##### 2.2.2、L1/L2正则化效果分析

###### 2.2.2.1、直观分析

考虑带约束条件的优化解释，对 L2 正则化为：
```mathjax!
$$
\underset{w}{min} J(w;X,y) \ \ \ \ \  s.t. ||w||_2 \le C
$$
```

![求解示意图](./images/1571405041243.png)

图中椭圆为原目标函数 `!$J(w)$` 的一条等高线，圆为半径 `!$\sqrt{C}$` 的 L2 范数球。由于约束条件的限制， `!$w$` 必须位于 L2 范数球内。考虑边界上的一点 `!$w$` ，图中蓝色箭头为 `!$J(w)$` 在该处的梯度方向 `!$\nabla J(w)$` ，红色箭头为 L2 范数球在该处的法线方向。由于 `!$w$` 不能离开边界（否则违反约束条件），因而在使用梯度下降法更新 `!$w$` 时，只能朝 `!$\nabla J(w)$` 在 范数球上`!$w$` 处的切线方向更新，即图中绿色箭头的方向。如此 `!$w$` 将沿着边界移动，当 `!$\nabla J(w)$` 与范数球上 `!$w$` 处的法线平行时，此时 `!$\nabla J(w)$` 在切线方向的分量为 0，`!$w$` 将无法继续移动，从而达到最优解 `!$w^*$`（图中红色点所示）。

对于 L1 正则化：
```mathjax!
$$
\underset{w}{min} J(w;X,y) \ \ \ \ \  s.t. ||w||_1 \le C
$$
```

![求解示意图](./images/1571405195502.png)

其主要差别在于 L1 、 L2  范数球的形状差异。由于此时每条边界上 `!$w$` 的切线和法线方向保持不变，在图中 `!$w$` 将一直朝着 `!$\nabla J(w)$` 在切线方向的分量沿着边界向左上移动。当 `!$w$` 跨过顶点到达 `!$w'$` 时， `!$\nabla J(w)$` 在切线方向的分量变为右上方，因而 `!$w$` 将朝右上方移动。最终， `!$w$` 将稳定在顶点处，达到最优解 `!$w^*$` 。此时，可以看到 `!$w_1 = 0$` ，这也就是采用 L1 范数会使 `!$w$` 产生稀疏性的原因。

以上分析虽是基于二维的情况，但不难将其推广到多维情况，其主要目的是为了直观地说明 L1 、 L2 正则化最优解的差异，以及L1 范数为什么为产生稀疏性。

###### 2.2.2.2、龙格现象

在数值分析领域中，龙格现象是在一组等间插值点上使用具有高次多项式的多项式插值时出现的区间边缘处的振荡问题。 它是由卡尔·龙格（Runge）在探索使用多项式插值逼近某些函数时的错误行为时发现的。这一发现非常重要，因为它表明使用高次多项式插值并不总能提高准确性。 该现象与傅里叶级数近似中的吉布斯现象相似。

![红色曲线是龙格函数，蓝色曲线是 5 阶多项式，绿色曲线是 9 阶多项式。随着阶次的增加，误差逐渐变大](./images/1571405395548.png)

**解决方法：**

使用切比雪夫节点代替等距点可以减小震荡，在这种情况下，随着多项式阶次的增加最大误差逐渐减小。这个现象表明高阶多项式通常不适合用于插值。使用分段多项式样条可以避免这个问题。如果要减小插值误差，那么可以增加构成样条的多项式的数目，而不必是增加多项式的阶次。

###### 2.2.2.3、理论分析
假设原目标函数 `!$\nabla J(w)$` 的最优解为 `!$w^*$` ，并假设其为二阶可导，将 `!$\nabla J(w)$`在 `!$w^*$` 处进行二阶泰勒展开有：
```mathjax!
$$
\hat{J}(w) = J(w^*) + \frac{1}{2}(w - w^*)^T H(w - w^*)
$$
```


##### 2.2.3、解的推导

在没有偏置的条件下，我们可以得到 L1 正则化的目标函数：
```mathjax!
$$
\tilde{J}(w; X, y) = J(w; X, y) + \alpha\|w\|_1  \tag{2.14}
$$
```
我们的目的是求得使目标函数取最小值的 `!$w^∗$`，上式对 `!$w$` 求导可得：
```mathjax!
$$
\nabla_w \tilde{J}(w; X, y) = \nabla_w J(w; X, y) + \alpha \cdot sign(w) \tag{2.15}
$$
```
其中若 `!$w>0$`，则 `!$sign(w)=1$`；若 `!$w<0$`，则 `!$sign(w)=−1$`；若 `!$w=0$`，则 `!$sign(w)=0$`。当 `!$\alpha=0$`，假设我们得到最优的目标解是 `!$w^∗$`，用秦勤公式在 `!$w^∗$` 处展开可以得到（要注意的`!$\nabla \tilde{J}(w^∗)=0$`）：
```mathjax!
$$
\tilde{J}(w; X, y) = \tilde{J}(w^*; X, y) + \frac{1}{2}(w - w^*)H(w-w^*) \tag{2.16}
$$
```
其中 `!$H$` 是关于 `!$w$` 的Hessian矩阵，为了得到更直观的解，我们简化 `!$H$`，假设 `!$H$` 这对角矩阵，则有：
```mathjax!
$$
H = diag([H_{1,1},H_{2,2}...H_{n,n}])  \tag{2.17}
$$
```
将上式代入到式(2.14)中可以得到，我们简化后的目标函数可以写成这样：
```mathjax!
$$
\tilde{J}(w;X,y)=\tilde{J}(w^*;X,y)+\sum_i\left[\frac{1}{2}H_{i,i}(w_i-w_i^*)^2 + \alpha_i|w_i| \right] \tag{2.18}
$$
```
从上式可以看出，`!$w$` 各个方向的导数是不相关的，所以可以分别独立求导并使之为 0，可得：
```mathjax!
$$
H_{i,i}(w_i-w_i^*)+\alpha \cdot sign(w_i)=0  \tag{2.19}
$$
```
先直接给出上式的解，再来看推导过程：
```mathjax!
$$
w_i = sign(w^*) \max\left\{ |w_i^*| - \frac{\alpha}{H_{i,i}},0 \right\}  \tag{2.20}
$$
```
从式(2.18)与式(2.19)可以得到两点：
1. 可以看到式(2.18)中的二次函数是关于 `!$w^∗$` 对称的，所以若要使式(2.18)最小，那么必有：`!$|w_i|<|w^*|$`，因为在二次函数值不变的程序下，这样可以使得 `!$\alpha|w_i|$` 更小。
2. `!$sign(w_i)=sign(w_i^*)$` 或 `!$w_i=0$`，因为在 `!$\alpha|w_i|$` 不变的情况下，`!$sign(w_i)=sign(w_i^*)$` 或 `!$w_i=0$`可以使式(2.18)更小。
由式(2.19)与上述的第2点：`!$sign(w_i)=sign(w_i^*)$` 可以得到：
```mathjax!
$$
\begin{split}
0 &= H_{i,i}(w_i-w_i^*)+\alpha \cdot sign(w_i^*) \cr
w_i &= w_i^* - \frac{\alpha}{H_{i,i}}sign(w_i^*) \cr
w_i &= sign(w_i^*)|w_i^*| - \frac{\alpha}{H_{i,i}}sign(w_i^*)\cr
 &=sign(w_i^*)(|w_i^*| - \frac{\alpha}{H_{i,i}}) \cr
\end{split}  \tag{2.21}
$$
```
我们再来看一下第2点：`!$sign(w_i)=sign(w_i^*)$` 或 `!$w_i=0$`，若 `!$|w_i^*| < \frac{\alpha}{H_{i,i}}$`，那么有 `!$sign(w_i)\neq sign(w_i^*)$`，所以这时有 `!$w1=0$`，由于可以直接得到解式(2.20)。
从这个解可以得到两个可能的结果：
1. 若 `!$|w**_i| \le \frac{\alpha}{H_{i,i}}$` ，正则化后目标中的 `!$w_i$` 的最优解是 `!$w_i=0$`。因为这个方向上 `!$J(w;X,y)$` 的影响被正则化的抵消了。
2. 若 `!$|w**_i| > \frac{\alpha}{H_{i,i}}$` ，正则化不会推最优解推向0，而是在这个方面上向原点移动了 `!$\frac{\alpha}{H_{i,i}}$` 的距离。


















<br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />

L2 正则化公式非常简单，直接在原来的损失函数基础上加上权重参数的平方和：
```mathjax!
$$
Cost(w) = cos t(f - y) + \lambda \sum_{i = 1}^n w_i^2
$$
```
其中，`!$cos t(f-y)$` 是未包含正则化项的训练样本误差，`!$\lambda$` 是正则化参数，是可以调节的。

###### 物理意义：

正则化的目的是限制参数过多或者过大，避免模型更加复杂。例如，使用多项式模型，如果使用 10 阶多项式，模型可能过于复杂，容易发生过拟合。所以，为了防止过拟合，我们可以将其高阶部分的权重 `!$w$` 限制为 0，这样，就相当于从高阶的形式转换为低阶。

为了达到这一目的，最直观的方法就是限制 w 的个数，但是这类条件属于 NP-hard 问题，求解非常困难。所以，只能选择另一个方法，限制参数w的大小，一般的做法是寻找更宽松的限定条件：
```mathjax!
$$
\sum_{i = 1}^n w_i^2 \le C
$$
```
上式是对 `!$w$` 的平方和做数值上界限定，即所有w 的平方和不超过参数 `!$C$`。这时候，我们的目标就转换为：最小化训练样本误差 `!$cost（f-y）$`，但是要遵循 `!$w$` 平方和小于 `!$C$` 的条件。也可以理解为，在所有参数的平方和不超过 C 的前提条件下，还使得样本的训练误差最小。

 下面这张图可以说明如何在限定条件下，对  `!$cost（f-y）$` 进行最小化的优化：
 
 ![正则化1](./images/正则化1.jpg)
 
 **注意：** 因为图片是来自于一位网友的博客，所以图中的字母和文中使用的稍有区别，这个地方仅有两个参数 `!$w_1$`，`!$w_2$`，所以正则化约束为：
```mathjax!
$$
w_1^2 + w_2^2 \le C
$$
```

在图像上展示就是一个圆而 `!$cos t(f - y) = f(x_1,x_2,w_1,w_2) - y$`

当然，这里的 `!$x_1,x_2,w_1,w_2$` 之间的关系的可能是一个较为复杂的函数关系，注意，这里的蓝色范围本质上是一个三维图的俯视图范围，因为 `!$X$` 轴为 `!$w_1$`，`!$Y$` 轴为 `!$w_2$`，`!$Z$` 轴为 `!$f(x_1,x_2,w_1,w_2)$`。本质上，`!$cost(f-y)=f(x1,x2,w1,w2)-y$` 和 `!$regularity = w_1^2 + w_2^2$` 都是参数 `!$w_1$`、`!$w_2$` 的函数。

如上图所示，蓝色椭圆区域是最小化 `!$cost(f-y)$` 区域，红色圆圈是 参数 w 的限定条件区域。在没有限定条件的情况下，一般使用梯度下降算法，在蓝色椭圆区域内会一直沿着 w 梯度的反方向前进，直到找到全局最优值即可。例如空间中有一点 w（图中紫色点），此时 `!$w$` 会沿着 `!$-\nabla E_{in}$` 的方向移动，如图中蓝色箭头(梯度方向)所示。但是，由于存在限定条件，`!$w$` 不能离开红色圆形区域，最多只能位于圆上边缘位置，沿着切线方向。w 的方向如图中红色箭头所示。

那么问题来了，存在限定条件，w 最终会在什么位置取得最优解呢？也就是说在满足限定条件的基础上，尽量让 `!$cost(f-y)$` 最小。

![正则化2](./images/正则化2_1.jpg)

从上面可以得知，如果不考虑正则化范围，运动的方向为“蓝色箭头”，但根据向量的分解，分解为红色箭头（逐渐朝正则化范围边界移动）和绿色剪头，那什么时候停止呢？因为有红色箭头分量的存在，迟早会到达边界，当在正则化项的边界相切的时候，就停止更新参数，我们经常看见这样一句话：代价函数在“模型代价”和“正则化约束”两项规则的综合作用下，正则化后的模型参数应该收敛在“误差等值线”与“正则项等值线”的相切的位置。下面会解释它的含义。

<div style="text-align: center;">
<div style="width: 500px;height: 500px;display: inline-block;">

![正则化3](./images/正则化3.jpg)
</div>
</div>

如上图所示：红色表示的是“正则项等值线”，蓝色的表示的是 `!$cost(f-y)$` 的等值线，不一定是圆，这里用圆表示。在没有正则化项时，黑色箭头是梯度的下降方向，在有了正则化项以后，相当于给运动的箭头施加了约束，一方面，我不能逃离红色箭头的约束范围，另一方面，又要保证箭头要向着梯度减小的方向运动，故而运动的曲线是图中的绿色剪头。当红色箭头和绿色剪头相切的时候，不能够再继续向下运动了，因为它就逃离了红色范围的约束。
 
现在可以解释正则化“惩罚”二字的含义了。**它惩罚的是总的“成本函数”**，因为从图中可以看出，黑色箭头到达终点后的“代价函数”很明显是低于绿色剪头所到达的终点处的代价函数，即增加正则化项之后，代价适当增大了，即模型收到了“惩罚”。根据方差-偏差分解原则，偏差增大，方差会减小，方差减小，防止过拟合的能力增强。（注意：因为L2正则化会让参数趋于“平滑”，所以，有时候也说，惩罚大的参数，即让大的参数变小）

#### 2.2、L1正则化

L1正则化的原理与L2正则化类似，它的定义如下：
```mathjax!
$$
Cost(w) = cos t(f - y) + \lambda \sum_{i = 1}^n |w_i|
$$
```

根据 L2 的定义方式，则有：
```mathjax!
$$
\sum_{i = 1}^n |w_i| \le C
$$
```

此处依然以两个参数 `!$w_1$`，`!$w_2$` 为例，则有：
```mathjax!
$$
|w_1| + |w_2| \le C
$$
```

它的图形为一个正方形，如下所示：

<div style="text-align: center;">
<div style="width: 460px;height: 500px;display: inline-block;">

![正则化4](./images/正则化4.jpg)
</div>
</div>

`!$cos t(f-y)$` 优化算法不变，L1 正则化限定了 w 的有效区域是一个正方形，且满足 `!$|w| < C$`。空间中的点 w 沿着 `!$cost(f-y)$` 的方向移动。但是，与此同时 w 不能离开红色正方形区域，最多只能位于正方形边缘位置。其推导过程与 L2 类似，就不再重复说明了。

### 3、正则化的数学理解

（正则化与一些贝叶斯先验本质上是一致的，比如L2正则化与高斯先验是一致的、L1正则化与拉普拉斯先验是一致的）

#### 3.1、L2正则化的数学推导

我们前面从图形的角度，用了一些形象的描述去解释了L1、L2正则化的原理，那么怎么从数学角度的层面去解释为什么正则化可以解决过拟合呢？  
下面以L2正则化为例：
**成本函数为：**

```mathjax!
$$
Cost(w) = \frac{1}{2m}[\sum_{j = 1}^m f_w(x^{(j)}) - y^{(j)}]^2 + \lambda \sum_{i = 1}^n w_i^2
$$
```

在上面的式子中，m 表示的的是样本数，n 表示的是参数个数，不能混淆。从这里可以看出，此时的成本函数 Cost 不再仅仅由模型预测值和真实值之间的差别决定了，还可参数 w的取值有关。后面的正则化项是为了施加约束，现在要达到减小成本函数这一目的，就不能够随意的取w参数值了，为什么？

比如，某一很大的w确实能够使得成本函数前面的部分的取值很小，但是因为 `!$w_i^2$` 会很大，而最终组合起来的成本函数并不小，反而很大。这样，要使得总的成本函数很小，参数 w 就不能随意取多大的值了，不让 w 取太大的值，这不就是正则化的一种约束吗？需要约束 w 本身的大小，与此同时还要使得原来部分的误差减小，这就是 L2 达到的效果，这其实也就是 L2 正则项为什么使得参数w相对较平滑（即参数 w 大小相差不会太大），后面还会解释“平滑”二字的原因。

比如对于参数 `!$w_i$`，求偏导数，则有如下：
```mathjax!
$$
\frac{\partial Cost(w)}{\partial w_i} = \frac{1}{m} \sum_{j = 1}^m[(f(x^{(j)}) - y^{(j)})x_i^{(j)} + \frac{\lambda}{m}w_i]
$$
```

`!$\lambda$` 为正则项系数，所以有：
```mathjax!
$$

$$
```

https://zhuanlan.zhihu.com/p/29360425