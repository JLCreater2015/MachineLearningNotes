---
title: 梯度下降法
tags: 最优化算法,梯度下降法
grammar_cjkRuby: true
---

![enter description here](./images/td1.jpg)

#### 一、最优化问题

最优化问题是求解函数极值的问题，包括极大值和极小值。在机器学习之类的实际应用中，我们一般将最优化问题统一表述为求解函数的极小值问题，即：
```mathjax!
$$
min_xf(x)
$$
```
其中 `!$x$` 称为优化变量，`!$f$` 称为目标函数。极大值问题可以转换成极小值问题来求解，只需要将目标函数加上负号即可：
```mathjax!
$$
max_xf(x) \Leftrightarrow -min_xf(x)
$$
```
有些时候会对优化变量 `!$x$` 有约束，包括等式约束和不等式约束，它们定义了优化变量的可行域，即满足约束条件的点构成的集合。在这里我们先不考虑带约束条件的问题。

一个优化问题的全局极小值 `!$ x^{*} $`  是指对于可行域里所有的 `!$x$`，有：
```mathjax!
$$
f(x^{*}) \le f(x)
$$
```
即全局极小值点处的函数值不大于任意一点处的函数值。局部极小值 `!$ x^{*} $` 定义为存在一个`!$\delta $`  邻域，对于在邻域内：
```mathjax!
$$
||x - x^{*}|| \le \delta
$$
```
并且在可行域内的所有 `!$x$`，有：
```mathjax!
$$
f(x^{*}) \le f(x)
$$
```
即局部极小值点处的函数值比一个局部范围内所有点的函数值都小。在这里，我们的目标是找到全局极小值。不幸的是，有些函数可能有多个局部极小值点，因此即使找到了导数等于 0 的所有点，还需要比较这些点处的函数值。

#### 二、导数和梯度
实际应用中一般都是多元函数，梯度是导数对多元函数的推广，它是多元函数对各个自变量偏导数形成的向量。多元函数的梯度定义为：
```mathjax!
$$
\nabla f(x) = \left( \frac{\partial f}{\partial x_1},\ldots ,\frac{\partial f}{\partial x_n} \right)^T
$$
```
其中 `!$\nabla $` 称为梯度算子，它作用于一个多元函数，得到一个向量。下面是计算函数梯度的一个例子：
```mathjax!
$$
\nabla(x^2 + xy - y^2) = (2x + y,x - 2y)^T
$$
```
可导函数在某一点处取得极值的必要条件是梯度为 0，梯度为 0 的点称为函数的驻点，这是疑似极值点。需要注意的是，梯度为 0 只是函数取极值的必要条件而不是充分条件，即梯度为0的点可能不是极值点。

至于是极大值还是极小值，要看二阶导数 / Hessian矩阵，这是由函数的二阶偏导数构成的矩阵。这分为下面几种情况：

 - 如果Hessian矩阵正定，函数有极小值
 - 如果Hessian矩阵负定，函数有极大值
 - 如果Hessian矩阵不定，则需要进一步讨论

这和一元函数的结果类似，Hessian矩阵可以看做是一元函数的二阶导数对多元函数的推广。一元函数的极值判别法为，假设在某点处导数等于 0，则：

 - 如果二阶导数大于0，函数有极小值
 - 如果二阶导数小于0，函数有极大值
 - 如果二阶导数等于0，情况不定

直接求函数的导数 / 梯度，然后令导数 / 梯度为 0，解方程，问题不就解决了吗？事实上没这么简单，因为这个方程可能很难解。比如下面的函数：
```mathjax!
$$
f(x,y) = x^3 - 2x^2 + e^{xy} - y^3 + 10y^2 + 100sin(xy)
$$
```
我们分别对 x 和 y 求偏导数，并令它们为 0，得到下面的方程组：
```mathjax!
$$
\left\{ \begin{array}{c} 3x^2 - 4x + ye^{xy} + 100y cos(xy) = 0 \\ xe^{xy} - 3y^2 + 20y + x cos(xy) = 0 \end{array} \right.
$$
```
这个方程非常难以求解，对于有指数函数，对数函数，三角函数的方程，我们称为超越方程，求解的难度并不比求极值本身小。
 
精确的求解不太可能，因此只能求近似解，这称为数值计算。工程上实现时通常采用的是迭代法，它从一个初始点 `!$x_{0}$`  开始，反复使用某种规则从 `!$ x_{k}$`  移动到下一个点 `!$x_{k+1}$`  ，构造这样一个数列，直到收敛到梯度为 0 的点处。即有下面的极限成立：
```mathjax!
$$
lim_{k \rightarrow +\infty}\nabla f(x_k) = 0
$$
```
这些规则一般会利用一阶导数信息即梯度；或者二阶导数信息即Hessian矩阵。这样迭代法的核心是得到这样的由上一个点确定下一个点的迭代公式：
```mathjax!
$$
x_{k + 1} = h(x_k)
$$
```

#### 三、推导过程
首先我们来看一元函数的泰勒展开，以便于更好的理解多元函数的泰勒展开。如果一个一元函数 `!$n$` 阶可导，它的泰勒展开公式为：
```mathjax!
$$
f(x + \Delta x) = f(x) + f^{'}(x)\Delta x + \frac{1}{2} f^{''}{(\Delta x)}^2 + \cdots + \frac{1}{n!} f^{(n)}(x)(\Delta x)^n \ \ldots
$$
```
如果在某一点处导数值大于 0（+），则函数在此处是增函数，加大 x 的值函数值会增加，减小 x 的值函数值会减小。相反的，如果在某一点处导数值小于 0（-），则函数是减函数，增加 x 的值函数值会减小，减小 x 的值函数值会增加。因此我们可以得出一个结论：**如果 x 的变化很小，并且变化值与导数值反号**，则函数值下降。对于一元函数，x 的变化只有两个方向，要么朝左，要么朝右。

下面我们把这一结论推广到多元函数的情况。多元函数 `!$f(x) $`  在 x 点处的泰勒展开为：
```mathjax!
$$
f(x + \Delta x) = f(x) + (\nabla f(x))^T \Delta x + \omicron(\Delta x)
$$
```
这里我们忽略了二次及更高的项。其中，一次项是梯度向量 `!$\nabla f(x)$` 与自变量增量 `!$ \Delta x $` 的内积 `!$(\nabla f(x))^T \Delta x$`，这等价于一元函数的 `!$ f^{'}(x_{0})\Delta x $`  。这样，函数的增量与自变量的增量 `!$\Delta x$`、函数梯度的关系可以表示为：
```mathjax!
$$
f(x + \Delta x) - f(x) = (\nabla f(x))^T \Delta x + \omicron(\Delta x)
$$
```
如果 `!$\Delta x$`  足够小，在 x 的某一邻域内，则我们可以忽略二次及以上的项，有：
```mathjax!
$$
f(x + \Delta x) - f(x) \approx (\nabla f(x))^T \Delta x
$$
```
这里的情况比一元函数复杂多了， `!$ \Delta x $` 是一个向量，`!$\Delta x$` 有无穷多种方向，该往哪个方向走呢？如果能保证：
```mathjax!
$$
(\nabla f(x))^T \Delta x < 0
$$
```
则有：
```mathjax!
$$
f(x + \Delta x) < f(x)
$$
```
即函数值递减，这就是下山的正确方向。因为有：
```mathjax!
$$
(\nabla f(x))^T \Delta x = ||\nabla f(x)||\  ||\Delta x|| cos\theta
$$
```
在这里，`!$||·||$` 表示向量的模，`!$\theta $` 是向量 `!$\nabla f(x)$` 和 `!$\Delta x $` 的夹角。因为向量的模一定大于等于 0，如果：
```mathjax!
$$
cos \theta \le 0
$$
```
则能保证：
```mathjax!
$$
(\nabla f(x))^T \Delta x < 0
$$
```
即选择合适的增量 `!$ \Delta x $`，就能保证函数值下降，要达到这一目的，只要保证梯度和 `!$\Delta x $` 的夹角的余弦值小于等于 0 就可以了。由于有：
```mathjax!
$$
cos \theta \ge -1
$$
```
只有当：
```mathjax!
$$
\theta = \pi
$$
```
时 `!$cos\theta $` 有极小值 -1，此时梯度和 `!$\Delta x $` 反向，即夹角为 180 度。因此当向量`!$\Delta x $` 的模大小一定时，当：
```mathjax!
$$
\Delta x = - \alpha \nabla f(x)
$$
```
即在梯度相反的方向函数值下降的最快。此时有：
```mathjax!
$$
cos\theta = -1
$$
```
函数的下降值为：
```mathjax!
$$
{(\nabla f(x))}^T \Delta x = - ||\nabla f(x)|| \ ||\Delta x|| = -\alpha{||\nabla f(x)||}^2
$$
```
只要梯度不为 0，往梯度的反方向走函数值一定是下降的。直接用 `!$\Delta x = - \nabla f(x)$` 可能会有问题，因为 `!$x + \Delta x $` 可能会超出 x 的邻域范围之外，此时是不能忽略泰勒展开中的二次及以上的项的，因此步伐不能太大。一般设：
```mathjax!
$$
\Delta x = -\alpha \nabla f(x)
$$
```
其中 `!$\alpha $` 为一个接近于 0 的正数，称为步长，由人工设定，用于保证  `!$x + \Delta x $` 在 x 的邻域内，从而可以忽略泰勒展开中二次及更高的项，则有：
```mathjax!
$$
{(\nabla f(x))}^T \Delta x = -\alpha {(\nabla f(x))}^T (\nabla f(x)) \le 0
$$
```
从初始点 `!$x_{0}$` 开始，使用如下迭代公式：
```mathjax!
$$
x_{k + 1} = x_k - \alpha \nabla f(x_k)
$$
```
只要没有到达梯度为 0 的点，则函数值会沿着序列 `!$ x_{k}$` 递减，最终会收敛到梯度为 0 的点，这就是梯度下降法。迭代终止的条件是函数的梯度值为 0（实际实现时是接近于0），此时认为已经达到极值点。注意我们找到的是梯度为 0 的点，这不一定就是极值点，后面会说明。梯度下降法只需要计算函数在某些点处的梯度，实现简单，计算量小。

#### 四、实现细节问题
###### 初始值的设定

一般的，对于不带约束条件的优化问题，我们可以将初始值设置为 0，或者设置为随机数，对于神经网络的训练，一般设置为随机数，这对算法的收敛至关重要。

###### 学习率的设定

学习率设置为多少，也是实现时需要考虑的问题。最简单的，我们可以将学习率设置为一个很小的正数，如 0.001。另外，可以采用更复杂的策略，在迭代的过程中动态的调整学习率的值。比如前 1 万次迭代为 0.001，接下来 1 万次迭代时设置为 0.0001。

#### 五、面临的问题
在实现时，梯度下降法可能会遇到一些问题，典型的是局部极小值和鞍点问题。

###### 局部极小值
有些函数可能有多个局部极小值点，下面是一个例子：

![enter description here](./images/td2.jpg)

这张图中的函数有3个局部极值点，分别是A，B和C，但只有A才是全局极小值，梯度下降法可能迭代到B或者C点处就终止。

###### 鞍点 
鞍点是指梯度为0，Hessian矩阵既不是正定也不是负定，即不定的点。下面是鞍点的一个例子，假设有函数：
```mathjax!
$$
x^2 - y^2
$$
```
显然在(0, 0)这点处不是极值点，但梯度为0，下面是梯度下降法的运行结果：

![enter description here](./images/td3.jpg)

在这里，梯度下降法遇到了鞍点，认为已经找到了极值点，从而终止迭代过程，而这根本不是极值点。

#### 六、变种
梯度下降法有大量的变种，它们都只利用之前迭代时的梯度信息来构造每次的更新值。

###### 1、动量
最直接的改进是为迭代公式加上动量项，动量项累积了之前的权重更新值，加上此项之后的参数更新公式为：