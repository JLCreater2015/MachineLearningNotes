---
title: 熵与交叉熵
tags: 熵,交叉熵,逻辑回归,Softmax
grammar_cjkRuby: true
---

#### 一、熵
熵（entropy）是信息论中最基本、最核心的一个概念，它衡量了一个概率分布的随机程度，或者说包含的信息量的大小。首先来看离散型随机变量。考虑随机变量取某一个特定值时包含的信息量的大小。假设随机变量取值为 x，对应的概率为 p(x)。直观来看，取这个值的可能性越小，而它又发生了，则包含的信息量就越大。因此如果定义一个函数 h(x) 来描述随机变量取值为的信息量的大小的话，则 h(x) 应该是 p(x) 的单调减函数。例如，一年之内人类登陆火星，包含的信息量显然比广州明天要下雨大，因为前者的概率明显小于后者。

满足单调递减要求的函数太多了，我们该选择哪个函数呢？接着考虑。假设有两个**相互独立的随机变量**，它们的取值分别为和，取该值的概率为p(x)和p(y)。根据随机变量的独立性，它们的联合概率为：
```mathjax!
$$
p(x,y) = p(x)p(y)
$$
```
由于这两个随机变量是相互独立的，因此它们各自取某一值时包含的信息量应该是两个随机变量分别取这些值的时候包含的信息量之和：
```mathjax!
$$
h(x,y) = h(x) + h(y)
$$
```
这要求 h(x) 能把 p(x) 的乘法转化为加法，在数学上，满足此要求的是对数函数。因此，可以把信息量定义为：
```mathjax!
$$
h(x) = -log\ p(x)
$$
```
这个对数的底数是多少并没有太大关系，根据换底公式，最后计算出来的结果就差了一个倍数，信息论中通常以 2 为底，在机器学习中通常以 e 为底，在后面的计算中为了方便起见我们用 10 为底。需要强调的对数函数前面加上了负号，这是因为对数函数是增函数，而我们要求 h(x) 是 p(x) 的减函数。另外，由于0≤p(x)≤1，因此 logp(x)<0，加上负号之后刚好可以保证这个信息量为正。